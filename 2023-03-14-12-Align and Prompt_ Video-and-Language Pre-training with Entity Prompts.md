## Paper:54






1. Title: Align and Prompt: Video-and-Language Pre-training with Entity Prompts (使用实体提示的视频与语言预训练模型)

2. Authors: Xin Eric Wang, Yuning You, Roozbeh Mottaghi, Devi Parikh, Dhruv Batra

3. Affiliation: Salesforce Research (Salesforce 研究机构)

4. Keywords: Video-and-Language Pre-training, Entity Prompts, Cross-modal Alignment, Prompting Entity Modeling

5. Urls: http://arxiv.org/abs/2112.09583v2, Github: https://github.com/salesforce/ALPRO

6. Summary:

- (1): 本文的研究背景是视频与语言模态之间的复杂交互关系，以及以往视频与语言预训练模型在有效跨模态地捕捉这种交互方面存在的挑战。

- (2): 以往方法主要是使用基于transformer的多模态编码器捕捉跨模态交互，缺乏充分解决单模态视频和文本特征之间的错位问题。同时，如果要学习细粒度的视觉-语言对齐，则需要使用现成的物体检测器来提供对象信息，但这受限于检测器的有限语料库和昂贵的计算成本。本文提出了使用实体提示的视频与语言预训练模型Align and Prompt，该模型通过视频文本对比损失 (VTC loss) 对单模态视频文本特征进行对齐，使得跨模态交互的建模更容易。 为学习细粒度的区域-实体对齐，本文提出了一种新的基于视觉引导的预训练任务Prompting Entity Modeling (PEM)，该任务要求模型预测随机选择的视频裁剪的实体伪标签，并通过VTC loss训练实体提示模块，产生视频裁剪和实体提示之间的相似度。相比以往方法，阿里的方法在文本-视频检索和视频问答任务上取得了最先进的性能，且超越了以往方法较大的幅度。

- (3): 本文提出了一种新的实体提示模块，并结合视觉引导的预训练任务Prompting Entity Modeling (PEM)，用于对齐单模态视频与文本。该方法通过PEM任务要求模型预测随机选择的视频裁剪的实体伪标签，并通过VTC loss训练实体提示模块，产生视频裁剪和实体提示之间的相似度。

- (4): 基于本文提出的方法，实验结果在文本-视频检索和视频问答任务上取得了最先进的性能，且超越了以往方法较大的幅度，支持了提出的方法的目标。同时，本文提出的方法还无需使用物体检测器，比以往方法更具实用性和可扩展性。
7. Methods: 

- (1): 该研究提出了一种叫做Align and Prompt的视频与语言预训练模型，使用实体提示进行视频单模态和文本特征之间的对齐，通过最大化正样本视频-文本对之间的相似度，显式地捕捉跨模态交互的局部对齐。该模型通过视频文本对比损失 (VTC loss) 进行损失函数的优化，提高了模型的对跨模态交互建模的能力。

- (2): 为了学习细粒度的区域-实体对齐，本文提出了一种新的基于视觉引导的预训练任务叫做Prompting Entity Modeling (PEM)，该任务要求模型预测随机选择的视频裁剪的实体伪标签，并将其作为实体提示输入到Align and Prompt模型中，通过VTC loss进行参数优化，产生视频裁剪和实体提示之间的相似度。PEM任务的引入，可以更准确地捕捉视频单模态和文本之间的细粒度对齐，提高模型的推理能力。

- (3): Align and Prompt模型较之前的模型无需物体检测器等依赖于外部数据源的工具，具有更高的实用性和可扩展性，同时在文本-视频检索和视频问答任务上均取得了最先进的性能，证明了该方法的有效性。





8. Conclusion: 

- (1): 本文提出了使用实体提示的视频与语言预训练模型，通过视频文本对比损失进行跨模态交互的局部对齐，为视觉与语言领域的研究开辟了新方向。 

- (2): 创新点：引入实体提示并结合视觉引导的预训练任务，无需物体检测器等外部工具，极大地提高了模型的实用性和可扩展性。性能：在文本-视频检索和视频问答任务上均取得了最先进的性能，超越了以往方法较大的幅度。工作量：未涉及过多的实验设计和数据处理，具有较小的工作量。




