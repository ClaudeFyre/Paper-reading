## Paper:78




1. Title: Cross-Lingual Named Entity Recognition Using Parallel Corpus: A New Approach Using XLM-RoBERTa Alignment (使用平行语料库的跨语言命名实体识别：一种新的XLM-RoBERTa对齐方法)

2. Authors: Xiaofeng Ren, Kexin Wang, Xiaoye Tan, Jiashen Sun, Zhenghua Li

3. Affiliation: Renmin University of China (中国人民大学)

4. Keywords: Named entity recognition; Cross-lingual transfer; Parallel corpus; Machine learning; XLM-RoBERTa

5. Urls: Paper(http://arxiv.org/abs/2101.11112v1), Github(None)

6. Summary: 

- (1):本文的研究背景是跨语言命名实体识别，即如何在不同语言之间进行实体识别任务的迁移学习。

- (2):以前的方法主要有两种，一种是基于模型的迁移学习，如在源语言上训练命名实体识别模型，然后直接将其应用于目标语言。另一种是基于数据的迁移学习，即通过对齐平行语料库中的标注数据，构建目标语言的伪标注数据来训练模型。然而，这些方法存在诸如翻译不准确、噪声数据过多、需要大量人工特征工程等问题。本文提出了一种新的方法，可以利用预训练模型和平行语料库来进行实体对齐和迁移学习，避免了这些问题的影响。

- (3):本文提出的方法是建立在XLM-RoBERTa模型基础之上的。首先，在源语言上训练一个命名实体识别模型，在平行语料库上对源语言进行识别并进行实体对齐，然后用对齐后的实体标签来伪标注目标语言数据，最后在目标语言上对伪标注数据进行重新训练。同时，本文还引入了一种反向加权的损失函数来提高训练精度。

- (4):在实验中，本文的方法在4种不同的目标语言上均取得了竞争性的成果，优于许多现有的相似方法。这说明该方法可以有效地进行跨语言命名实体识别的迁移学习，并在实践中实现一定的商业价值。
7. Methods: 

- (1): 本文方法的核心思想是利用预训练模型和平行语料库来进行跨语言实体对齐和迁移学习。首先，在源语言上训练一个命名实体识别模型，然后在平行语料库上对源语言进行识别并进行实体对齐。具体地说，与源语言中的实体对应的目标语言标记被视为伪标注数据，由此产生的伪标注数据可以在目标语言上进行重新训练。

- (2): 为了提高训练精度，本文还引入了一种反向加权的损失函数。损失函数在两个方面增加了反向加权项：对实体识别错误损失进行惩罚，并对实体对齐进行鼓励。这样，可以减少伪标注数据中错误标签的贡献，提高识别和对齐的准确性。

- (3): 本文的主要实验是基于4种不同的目标语言进行的，分别是德语、法语、意大利语和西班牙语。结果表明，本文提出的方法在所有语言条件下均优于对照组，并且与其他最先进的方法相比，本文的方法对于实体对齐和跨语言迁移学习具有更好的性能。





8. Conclusion: 

- (1): 本文的意义在于提出了一种有效的跨语言命名实体识别方法，通过利用平行语料库和预训练模型实现了实体的对齐和迁移学习，具有很高的商业应用价值。

- (2): 创新点：本文利用XLM-RoBERTa模型进行跨语言命名实体识别，引入反向加权损失函数提高训练精度；表现：在4种不同的目标语言上均取得了竞争性的成果，优于许多现有的相似方法；工作量：该方法可以自动化地进行，从而避免了大量人工工作。




